{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/09/07 14:42:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://master:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>SQL Interview Questions</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fa4fc304df0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, IntegerType, DateType\n",
    "from pyspark.sql import Window as W\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SQL Interview Questions\") \\\n",
    "    .getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|word   |count(1)|\n",
      "+-------+--------+\n",
      "|Hi     |1       |\n",
      "|a      |1       |\n",
      "|count  |1       |\n",
      "|am     |2       |\n",
      "|program|1       |\n",
      "|using  |1       |\n",
      "|spark  |1       |\n",
      "|df     |1       |\n",
      "|writing|1       |\n",
      "|i      |2       |\n",
      "|durai. |1       |\n",
      "|word   |1       |\n",
      "+-------+--------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Hi', 1),\n",
       " ('i', 2),\n",
       " ('using', 1),\n",
       " ('df', 1),\n",
       " ('a', 1),\n",
       " ('word', 1),\n",
       " ('program', 1),\n",
       " ('spark', 1),\n",
       " ('am', 2),\n",
       " ('writing', 1),\n",
       " ('count', 1),\n",
       " ('durai.', 1)]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Wordcount problem\n",
    "from pyspark.sql import functions as F \n",
    "df = spark.createDataFrame([\"Hi i am durai. i am writing a word count program using spark df\"],\"string\")\n",
    "df.select(F.explode(F.split(\"value\",\" \")).alias(\"word\")).groupBy(\"word\").agg(F.count(F.lit(1))).show(1000,False)\n",
    "\n",
    "#RDD\n",
    "rdd = df.rdd\n",
    "words_rdd = rdd.flatMap(lambda line: line.value.split(\" \"))\n",
    "word_pairs_rdd = words_rdd.map(lambda word: (word, 1))\n",
    "word_counts_rdd = word_pairs_rdd.reduceByKey(lambda a, b: a + b)\n",
    "word_counts_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+--------+\n",
      "| id| name|age|    city|\n",
      "+---+-----+---+--------+\n",
      "|  1|Alice| 30|New York|\n",
      "+---+-----+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Q1: Different delimiter \n",
    "# \n",
    "from pyspark.sql.functions import split, col\n",
    "data = [\"1,Alice\\t30|New York\"]\n",
    "\n",
    "# Creating a DataFrame with a single column\n",
    "df = spark.createDataFrame(data, \"string\")\n",
    "\n",
    "# Custom logic to split the mixed delimiter row\n",
    "split_col = split(df['value'], ',|\\t|\\|')\n",
    "\n",
    "# Creating new columns for each split part\n",
    "df = df.withColumn('id', split_col.getItem(0))\\\n",
    "       .withColumn('name', split_col.getItem(1))\\\n",
    "       .withColumn('age', split_col.getItem(2))\\\n",
    "       .withColumn('city', split_col.getItem(3))\n",
    "\n",
    "# Selecting and showing the result\n",
    "df.select('id', 'name', 'age', 'city').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+------------+\n",
      "|      Date|ProductID|QuantitySold|\n",
      "+----------+---------+------------+\n",
      "|2023-01-01|      100|          10|\n",
      "|2023-01-02|      100|          15|\n",
      "|2023-01-03|      100|          20|\n",
      "|2023-01-04|      100|          25|\n",
      "|2023-01-05|      100|          30|\n",
      "|2023-01-06|      100|          35|\n",
      "|2023-01-07|      100|          40|\n",
      "|2023-01-08|      100|          45|\n",
      "+----------+---------+------------+\n",
      "\n",
      "+----------+---------+------------+----+\n",
      "|      Date|ProductID|QuantitySold| avg|\n",
      "+----------+---------+------------+----+\n",
      "|2023-01-01|      100|          10|10.0|\n",
      "|2023-01-02|      100|          15|12.5|\n",
      "|2023-01-03|      100|          20|15.0|\n",
      "|2023-01-04|      100|          25|17.5|\n",
      "|2023-01-05|      100|          30|20.0|\n",
      "|2023-01-06|      100|          35|22.5|\n",
      "|2023-01-07|      100|          40|25.0|\n",
      "|2023-01-08|      100|          45|30.0|\n",
      "+----------+---------+------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Moving avg\n",
    "from pyspark.sql import Row\n",
    "data = [Row(Date='2023-01-01', ProductID=100, QuantitySold=10),\n",
    "        Row(Date='2023-01-02', ProductID=100, QuantitySold=15),\n",
    "        Row(Date='2023-01-03', ProductID=100, QuantitySold=20),\n",
    "        Row(Date='2023-01-04', ProductID=100, QuantitySold=25),\n",
    "        Row(Date='2023-01-05', ProductID=100, QuantitySold=30),\n",
    "        Row(Date='2023-01-06', ProductID=100, QuantitySold=35),\n",
    "        Row(Date='2023-01-07', ProductID=100, QuantitySold=40),\n",
    "        Row(Date='2023-01-08', ProductID=100, QuantitySold=45)]\n",
    "\n",
    "# Create DataFrame\n",
    "df_sales = spark.createDataFrame(data)\n",
    "df_sales.show()\n",
    "df_sales.createOrReplaceTempView(\"Sales\")\n",
    "spark.sql(\"select *, avg(QuantitySold) over(partition by ProductID order by Date rows between 6 preceding and current row) as avg from Sales\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+\n",
      "|event_date|event_status|\n",
      "+----------+------------+\n",
      "|01-06-2020|         Won|\n",
      "|02-06-2020|         Won|\n",
      "|03-06-2020|         Won|\n",
      "|04-06-2020|        Lost|\n",
      "|05-06-2020|        Lost|\n",
      "|06-06-2020|        Lost|\n",
      "|07-06-2020|         Won|\n",
      "|08-06-2020|        Lost|\n",
      "+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"event_date\", StringType(), True),\n",
    "    StructField(\"event_status\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create data\n",
    "data = [\n",
    "    (\"01-06-2020\", \"Won\"),\n",
    "    (\"02-06-2020\", \"Won\"),\n",
    "    (\"03-06-2020\", \"Won\"),\n",
    "    (\"04-06-2020\", \"Lost\"),\n",
    "    (\"05-06-2020\", \"Lost\"),\n",
    "    (\"06-06-2020\", \"Lost\"),\n",
    "    (\"07-06-2020\", \"Won\"),\n",
    "    (\"08-06-2020\", \"Lost\")\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "# Show DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/06 17:43:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/06 17:43:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/06 17:43:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/06 17:43:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/06 17:43:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/06 17:43:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/06 17:43:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/06 17:43:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Stage 111:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+----------+\n",
      "|event_status|    min_dt|    max_dt|\n",
      "+------------+----------+----------+\n",
      "|         Won|01-06-2020|03-06-2020|\n",
      "|        Lost|04-06-2020|06-06-2020|\n",
      "|         Won|07-06-2020|07-06-2020|\n",
      "|        Lost|08-06-2020|08-06-2020|\n",
      "+------------+----------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "spark.sql(\"\"\"\n",
    "with df1 as (\n",
    "select *, if(lag(event_status) over(order by event_date) != event_status, 1, 0)  as nxt from df\n",
    ") ,\n",
    "df2 as (\n",
    "    select *, sum(nxt) over(order by event_date) as sum from df1\n",
    ") \n",
    "select distinct \n",
    "event_status,\n",
    "min(event_date) over(partition by sum) as min_dt,\n",
    "max(event_date) over(partition by sum) as max_dt\n",
    "from df2\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|student|\n",
      "+---+-------+\n",
      "|  1|  Alice|\n",
      "|  2|    Bob|\n",
      "|  3|Charlie|\n",
      "|  4|  David|\n",
      "|  5|    Eve|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exchange Seats of Students\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"student\", StringType(), True)\n",
    "])\n",
    "\n",
    "data = [\n",
    "    (1, \"Alice\"),\n",
    "    (2, \"Bob\"),\n",
    "    (3, \"Charlie\"),\n",
    "    (4, \"David\"),\n",
    "    (5, \"Eve\")\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()\n",
    "df.createOrReplaceTempView(\"df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|    res|\n",
      "+---+-------+\n",
      "|  1|    Bob|\n",
      "|  2|  Alice|\n",
      "|  3|  David|\n",
      "|  4|Charlie|\n",
      "|  5|    Eve|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spark.sql(\"\"\"\n",
    "select id, \n",
    "nvl(if (id %2 = 0, lag(student) over(partition by 1 order by id) ,lead(student) over(partition by 1 order by id)),student) as res from df\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+\n",
      "|from_id|to_id|duration|\n",
      "+-------+-----+--------+\n",
      "|     10|   20|      58|\n",
      "|     20|   10|      12|\n",
      "|     10|   30|      20|\n",
      "|     30|   40|     100|\n",
      "|     30|   40|     200|\n",
      "|     30|   40|     200|\n",
      "|     40|   30|     500|\n",
      "+-------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Number of Calls and Total Duration\n",
    "\n",
    "data = [\n",
    "    (10, 20, 58),\n",
    "    (20, 10, 12),\n",
    "    (10, 30, 20),\n",
    "    (30, 40, 100),\n",
    "    (30, 40, 200),\n",
    "    (30, 40, 200),\n",
    "    (40, 30, 500)\n",
    "]\n",
    "\n",
    "# Define schema for DataFrame\n",
    "columns = [\"from_id\", \"to_id\", \"duration\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema=columns)\n",
    "\n",
    "df.show()\n",
    "df.createOrReplaceTempView(\"df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 150:============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+-------------+\n",
      "|from_id|to_id|count(1)|sum(duration)|\n",
      "+-------+-----+--------+-------------+\n",
      "|     10|   20|       2|           70|\n",
      "|     10|   30|       1|           20|\n",
      "|     30|   40|       4|         1000|\n",
      "+-------+-----+--------+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select \n",
    "if(from_id > to_id, to_id,from_id) from_id,\n",
    "if(from_id > to_id, from_id,to_id) to_id,\n",
    "count(1),\n",
    "sum(duration)\n",
    "from \n",
    "df\n",
    "group by 1,2\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|player_id|player_name|\n",
      "+---------+-----------+\n",
      "|        1|      Nadal|\n",
      "|        2|    Federer|\n",
      "|        3|      Novak|\n",
      "+---------+-----------+\n",
      "\n",
      "+----+---------+-------+-------+-------+\n",
      "|year|Wimbledon|Fr_open|US_open|Au_open|\n",
      "+----+---------+-------+-------+-------+\n",
      "|2017|        2|      1|      1|      2|\n",
      "|2018|        3|      1|      3|      2|\n",
      "|2019|        3|      1|      1|      3|\n",
      "+----+---------+-------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data1 = [\n",
    "    (1, \"Nadal\"),\n",
    "    (2, \"Federer\"),\n",
    "    (3, \"Novak\")\n",
    "]\n",
    "col1 = [\"player_id\", \"player_name\"]\n",
    "\n",
    "data2 = [\n",
    "    (2017, 2, 1, 1, 2),\n",
    "    (2018, 3, 1, 3, 2) ,\n",
    "    (2019, 3, 1, 1, 3)\n",
    "]\n",
    "col2 = [\"year\", \"Wimbledon\", \"Fr_open\", \"US_open\", \"Au_open\"]\n",
    "\n",
    "df1 = spark.createDataFrame(data1, schema=col1)\n",
    "df2 = spark.createDataFrame(data2, schema=col2)\n",
    "\n",
    "df1.show()\n",
    "df2.show()\n",
    "df1.createOrReplaceTempView(\"df1\")\n",
    "df2.createOrReplaceTempView(\"df2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+--------+\n",
      "|player_id|player_name|count(1)|\n",
      "+---------+-----------+--------+\n",
      "|        2|    Federer|       3|\n",
      "|        1|      Nadal|       5|\n",
      "|        3|      Novak|       4|\n",
      "+---------+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "with awards as (select year, Wimbledon as player_id from df2 union all\n",
    "select year, Fr_open from df2 union all\n",
    "select year, US_open from df2 union all\n",
    "select year, Au_open from df2)\n",
    "select df1.player_id,player_name, count(*) from df1 left join awards on df1.player_id = awards.player_id group by 1,2\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-------+\n",
      "|EMPLOYEE|     DATES| STATUS|\n",
      "+--------+----------+-------+\n",
      "|      A1|2024-01-01|PRESENT|\n",
      "|      A1|2024-01-02|PRESENT|\n",
      "|      A1|2024-01-03|PRESENT|\n",
      "|      A1|2024-01-04| ABSENT|\n",
      "|      A1|2024-01-05|PRESENT|\n",
      "|      A1|2024-01-06|PRESENT|\n",
      "|      A1|2024-01-07| ABSENT|\n",
      "|      A1|2024-01-08| ABSENT|\n",
      "|      A1|2024-01-09| ABSENT|\n",
      "|      A1|2024-01-10|PRESENT|\n",
      "|      A2|2024-01-06|PRESENT|\n",
      "|      A2|2024-01-07|PRESENT|\n",
      "|      A2|2024-01-08| ABSENT|\n",
      "|      A2|2024-01-09|PRESENT|\n",
      "|      A2|2024-01-10| ABSENT|\n",
      "+--------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (\"A1\", \"2024-01-01\", \"PRESENT\"),\n",
    "    (\"A1\", \"2024-01-02\", \"PRESENT\"),\n",
    "    (\"A1\", \"2024-01-03\", \"PRESENT\"),\n",
    "    (\"A1\", \"2024-01-04\", \"ABSENT\"),\n",
    "    (\"A1\", \"2024-01-05\", \"PRESENT\"),\n",
    "    (\"A1\", \"2024-01-06\", \"PRESENT\"),\n",
    "    (\"A1\", \"2024-01-07\", \"ABSENT\"),\n",
    "    (\"A1\", \"2024-01-08\", \"ABSENT\"),\n",
    "    (\"A1\", \"2024-01-09\", \"ABSENT\"),\n",
    "    (\"A1\", \"2024-01-10\", \"PRESENT\"),\n",
    "    (\"A2\", \"2024-01-06\", \"PRESENT\"),\n",
    "    (\"A2\", \"2024-01-07\", \"PRESENT\"),\n",
    "    (\"A2\", \"2024-01-08\", \"ABSENT\"),\n",
    "    (\"A2\", \"2024-01-09\", \"PRESENT\"),\n",
    "    (\"A2\", \"2024-01-10\", \"ABSENT\")\n",
    "]\n",
    "\n",
    "# Define schema for the DataFrame\n",
    "columns = [\"EMPLOYEE\", \"DATES\", \"STATUS\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema=columns)\n",
    "df.createOrReplaceTempView(\"emp\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 253:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+----------+-------+\n",
      "|EMPLOYEE|       frm|        to| STATUS|\n",
      "+--------+----------+----------+-------+\n",
      "|      A1|2024-01-01|2024-01-03|PRESENT|\n",
      "|      A1|2024-01-04|2024-01-04| ABSENT|\n",
      "|      A1|2024-01-05|2024-01-06|PRESENT|\n",
      "|      A1|2024-01-07|2024-01-09| ABSENT|\n",
      "|      A1|2024-01-10|2024-01-10|PRESENT|\n",
      "|      A2|2024-01-06|2024-01-07|PRESENT|\n",
      "|      A2|2024-01-08|2024-01-08| ABSENT|\n",
      "|      A2|2024-01-09|2024-01-09|PRESENT|\n",
      "|      A2|2024-01-10|2024-01-10| ABSENT|\n",
      "+--------+----------+----------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "with cte as (\n",
    "select *, \n",
    "sum(if(lag(STATUS) over(partition by EMPLOYEE order by DATES) != STATUS, 1, 0)) over(partition by EMPLOYEE order by DATES) as chk \n",
    "from emp )\n",
    "select distinct \n",
    "EMPLOYEE ,\n",
    "min(DATES) over (partition by EMPLOYEE,chk) as frm,\n",
    "max(DATES) over (partition by EMPLOYEE,chk) as to,\n",
    "STATUS\n",
    "from cte\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 307:============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+----------+-------+\n",
      "|employee| from_date|   to_date| status|\n",
      "+--------+----------+----------+-------+\n",
      "|      A1|2024-01-01|2024-01-03|PRESENT|\n",
      "|      A1|2024-01-04|2024-01-04| ABSENT|\n",
      "|      A1|2024-01-05|2024-01-06|PRESENT|\n",
      "|      A1|2024-01-07|2024-01-09| ABSENT|\n",
      "|      A1|2024-01-10|2024-01-10|PRESENT|\n",
      "|      A2|2024-01-06|2024-01-07|PRESENT|\n",
      "|      A2|2024-01-08|2024-01-08| ABSENT|\n",
      "|      A2|2024-01-09|2024-01-09|PRESENT|\n",
      "|      A2|2024-01-10|2024-01-10| ABSENT|\n",
      "+--------+----------+----------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "with cte as \n",
    "\t\t(select *, row_number() over(partition by employee order by employee, dates) as rn \n",
    "\t\tfrom emp),\n",
    "\tcte_present as\n",
    "\t\t(select *, row_number() over(partition by employee order by employee, dates)\n",
    "\t\t, rn - row_number() over(partition by employee order by employee, dates) as flag\n",
    "\t\tfrom cte where status='PRESENT' ),\n",
    "\tcte_absent as\n",
    "\t\t(select *, row_number() over(partition by employee order by employee, dates)\n",
    "\t\t, rn - row_number() over(partition by employee order by employee, dates) as flag\n",
    "\t\tfrom cte where status='ABSENT' )\n",
    "select employee \n",
    ", min(dates) over(partition by employee, flag) as from_date \n",
    ", max(dates) over(partition by employee, flag) as to_date \n",
    ", status\t\t\t\t\t\t\n",
    "from cte_present\n",
    "union \n",
    "select  employee \n",
    ", min(dates) over(partition by employee, flag) as from_date \n",
    ", max(dates) over(partition by employee, flag) as to_date \n",
    ", status\t\t\t\t\t\t\n",
    "from cte_absent\n",
    "order by employee, from_date\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+\n",
      "| ID|CAPACITY_KG|\n",
      "+---+-----------+\n",
      "|  1|        300|\n",
      "|  2|        350|\n",
      "+---+-----------+\n",
      "\n",
      "+--------------+---------+-------+\n",
      "|PASSENGER_NAME|WEIGHT_KG|LIFT_ID|\n",
      "+--------------+---------+-------+\n",
      "|         Rahul|       85|      1|\n",
      "|        Adarsh|       73|      1|\n",
      "|          Riti|       95|      1|\n",
      "|       Dheeraj|       80|      1|\n",
      "|         Vimal|       83|      2|\n",
      "|          Neha|       77|      2|\n",
      "|         Priti|       73|      2|\n",
      "|      Himanshi|       85|      2|\n",
      "+--------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "lift_schema = StructType([\n",
    "    StructField(\"ID\", IntegerType(), True),\n",
    "    StructField(\"CAPACITY_KG\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "lift_data = [(1, 300), (2, 350)]\n",
    "df_lift = spark.createDataFrame(lift_data, schema=lift_schema)\n",
    "df_lift.show()\n",
    "df_lift.createOrReplaceTempView(\"lift\")\n",
    "\n",
    "passengers_schema = StructType([\n",
    "    StructField(\"PASSENGER_NAME\", StringType(), True),\n",
    "    StructField(\"WEIGHT_KG\", IntegerType(), True),\n",
    "    StructField(\"LIFT_ID\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "passengers_data = [\n",
    "    (\"Rahul\", 85, 1),\n",
    "    (\"Adarsh\", 73, 1),\n",
    "    (\"Riti\", 95, 1),\n",
    "    (\"Dheeraj\", 80, 1),\n",
    "    (\"Vimal\", 83, 2),\n",
    "    (\"Neha\", 77, 2),\n",
    "    (\"Priti\", 73, 2),\n",
    "    (\"Himanshi\", 85, 2)\n",
    "]\n",
    "\n",
    "df_passengers = spark.createDataFrame(passengers_data, schema=passengers_schema)\n",
    "df_passengers.show()\n",
    "df_passengers.createOrReplaceTempView(\"passengers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------------------------------+\n",
      "|LIFT_ID|concat_ws( , , collect_list(PASSENGER_NAME))|\n",
      "+-------+--------------------------------------------+\n",
      "|1      |Adarsh , Dheeraj , Rahul                    |\n",
      "|2      |Priti , Neha , Vimal , Himanshi             |\n",
      "+-------+--------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "with cte as (select *, if(sum(WEIGHT_KG) over(partition by ID order by WEIGHT_KG) > CAPACITY_KG, 1, 0) as fl from passengers join lift on LIFT_ID = ID)\n",
    "select LIFT_ID, concat_ws(\" , \", collect_list(PASSENGER_NAME)) from cte where fl = 0 group by 1\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|    TIME|STATUS|\n",
      "+--------+------+\n",
      "|10:00:00|    on|\n",
      "|10:01:00|    on|\n",
      "|10:02:00|    on|\n",
      "|10:03:00|   off|\n",
      "|10:04:00|    on|\n",
      "|10:05:00|    on|\n",
      "|10:06:00|   off|\n",
      "|10:07:00|   off|\n",
      "|10:08:00|   off|\n",
      "|10:09:00|    on|\n",
      "|10:10:00|    on|\n",
      "|10:11:00|    on|\n",
      "|10:12:00|    on|\n",
      "|10:13:00|   off|\n",
      "|10:14:00|   off|\n",
      "|10:15:00|    on|\n",
      "|10:16:00|   off|\n",
      "|10:17:00|   off|\n",
      "+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"TIME\", StringType(), True),\n",
    "    StructField(\"STATUS\", StringType(), True)\n",
    "])\n",
    "\n",
    "data = [\n",
    "    (\"10:00:00\", \"on\"),\n",
    "    (\"10:01:00\", \"on\"),\n",
    "    (\"10:02:00\", \"on\"),\n",
    "    (\"10:03:00\", \"off\"),\n",
    "    (\"10:04:00\", \"on\"),\n",
    "    (\"10:05:00\", \"on\"),\n",
    "    (\"10:06:00\", \"off\"),\n",
    "    (\"10:07:00\", \"off\"),\n",
    "    (\"10:08:00\", \"off\"),\n",
    "    (\"10:09:00\", \"on\"),\n",
    "    (\"10:10:00\", \"on\"),\n",
    "    (\"10:11:00\", \"on\"),\n",
    "    (\"10:12:00\", \"on\"),\n",
    "    (\"10:13:00\", \"off\"),\n",
    "    (\"10:14:00\", \"off\"),\n",
    "    (\"10:15:00\", \"on\"),\n",
    "    (\"10:16:00\", \"off\"),\n",
    "    (\"10:17:00\", \"off\")\n",
    "]\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "df.show()\n",
    "df.createOrReplaceTempView(\"df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------+\n",
      "|logon   |logoff  |duration|\n",
      "+--------+--------+--------+\n",
      "|10:00:00|10:03:00|3.0     |\n",
      "|10:04:00|10:06:00|2.0     |\n",
      "|10:09:00|10:13:00|4.0     |\n",
      "|10:15:00|10:16:00|1.0     |\n",
      "+--------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "with cte as (select *, lag(STATUS, 1, 'off') over(partition by 1 order by TIME) as fl from df),\n",
    "cte2 as (select * from cte where STATUS != fl),\n",
    "cte3 as (select *, row_number() over(partition by STATUS order by TIME) as rnk from cte2)\n",
    "select min(TIME) logon, max(TIME) logoff, (unix_timestamp(max(TIME),'HH:mm:ss') - unix_timestamp(min(TIME),'HH:mm:ss') ) / 60 duration \n",
    "from cte3 group by rnk\n",
    "\"\"\").show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
