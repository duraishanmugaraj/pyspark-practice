{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/09/10 17:04:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://master:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>30 Days SQL</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f15b04e6df0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, IntegerType, DateType\n",
    "from pyspark.sql import Window as W\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"30 Days SQL\") \\\n",
    "    .getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Day 1 (https://www.youtube.com/watch?v=FRzbOb3jdLg) \"HARD\"\n",
    "# Define schema based on the table structure\n",
    "schema = StructType([\n",
    "    StructField(\"brand1\", StringType(), True),\n",
    "    StructField(\"brand2\", StringType(), True),\n",
    "    StructField(\"year\", IntegerType(), True),\n",
    "    StructField(\"custom1\", IntegerType(), True),\n",
    "    StructField(\"custom2\", IntegerType(), True),\n",
    "    StructField(\"custom3\", IntegerType(), True),\n",
    "    StructField(\"custom4\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Data from the image's table\n",
    "data = [\n",
    "    (\"apple\", \"samsung\", 2020, 1, 2, 1, 2),\n",
    "    (\"samsung\", \"apple\", 2020, 1, 2, 1, 2),\n",
    "    (\"apple\", \"samsung\", 2021, 1, 2, 5, 3),\n",
    "    (\"samsung\", \"apple\", 2021, 5, 3, 1, 2),\n",
    "    (\"google\", \"\", 2020, 5, 9, None, None),\n",
    "    (\"oneplus\", \"nothing\", 2020, 5, 9, 6, 3),\n",
    "]\n",
    "\n",
    "# Create DataFrame using the data and schema defined above\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "df.createOrReplaceTempView(\"Sales\")\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "with cte as (\n",
    "    select \n",
    "    least(brand1,brand2) as brand1, \n",
    "    greatest(brand1,brand2) as brand2,\n",
    "    year,custom1,custom2,custom3,custom4\n",
    "    from sales)\n",
    ",\n",
    "cte2 as (\n",
    "    select *, \n",
    "    case \n",
    "    when (custom1 = custom3) and (custom2 = custom4)  then 1 \n",
    "    when (custom1 != custom3) or (custom2 != custom4) then 0 \n",
    "    end as fl \n",
    "    from cte)\n",
    ",\n",
    "cte3 as (\n",
    "    select *, \n",
    "    if(fl = 1, row_number() over(partition by  brand1, brand2, year, fl order by brand1, brand2, year, fl), 0) as rnk\n",
    "    from cte2)\n",
    "select * from cte3 where rnk <=1\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Day2 https://www.youtube.com/watch?v=FRzbOb3jdLg\n",
    "\n",
    "mountain_huts_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"altitude\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Data for mountain_huts DataFrame\n",
    "mountain_huts_data = [\n",
    "    (1, 'Dakonat', 1900),\n",
    "    (2, 'Natisa', 2100),\n",
    "    (3, 'Gajantut', 1600),\n",
    "    (4, 'Rifat', 782),\n",
    "    (5, 'Tupur', 1370)\n",
    "]\n",
    "\n",
    "# Create DataFrame for mountain_huts\n",
    "df_mountain_huts = spark.createDataFrame(data=mountain_huts_data, schema=mountain_huts_schema)\n",
    "df_mountain_huts.show()\n",
    "df_mountain_huts.createOrReplaceTempView(\"mountain_huts\")\n",
    "\n",
    "# Define schema for trails DataFrame\n",
    "trails_schema = StructType([\n",
    "    StructField(\"hut1\", IntegerType(), True),\n",
    "    StructField(\"hut2\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Data for trails DataFrame\n",
    "trails_data = [\n",
    "    (1, 3),\n",
    "    (3, 2),\n",
    "    (3, 5),\n",
    "    (4, 5),\n",
    "    (1, 5)\n",
    "]\n",
    "\n",
    "# Create DataFrame for trails\n",
    "df_trails = spark.createDataFrame(data=trails_data, schema=trails_schema)\n",
    "df_trails.show()\n",
    "df_trails.createOrReplaceTempView(\"trails\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "with cte as (\n",
    "    select \n",
    "    hut1 as start, m1.name as start_name, m1.altitude as start_altitude ,\n",
    "    hut2 as end, m2.name as end_name, m2.altitude as end_altitude \n",
    "    from trails t1 \n",
    "    left join \n",
    "    mountain_huts m1 on\n",
    "    t1.hut1 = m1.id\n",
    "    left join \n",
    "    mountain_huts m2 on\n",
    "    t1.hut2 = m2.id)\n",
    ",\n",
    "cte2 as (\n",
    "    select \n",
    "    if(start_altitude > end_altitude, end, start) as start,\n",
    "    if(start_altitude > end_altitude, end_name, start_name) as start_name,\n",
    "    if(start_altitude > end_altitude, end_altitude, start_altitude) as start_altitude,\n",
    "    if(start_altitude > end_altitude, start, end) as end,\n",
    "    if(start_altitude > end_altitude, start_name, end_name) as end_name,\n",
    "    if(start_altitude > end_altitude, start_altitude, end_altitude) as end_altitude\n",
    "    from cte)\n",
    ",\n",
    "cte3 as (\n",
    "    select t1.start, t1.start_name, t1.start_altitude, t1.end, t1.end_name, t1.end_altitude, t2.end, t2.end_name, t2.end_altitude \n",
    "    from cte2 t1 \n",
    "    inner join \n",
    "    cte2 t2 on t1.end = t2.start \n",
    ")\n",
    "select * from cte3\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "footer_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"car\", StringType(), True),\n",
    "    StructField(\"length\", IntegerType(), True),\n",
    "    StructField(\"width\", IntegerType(), True),\n",
    "    StructField(\"height\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Data for FOOTER DataFrame\n",
    "footer_data = [\n",
    "    (1, 'Hyundai Tucson', 15, 6, None),\n",
    "    (2, None, None, None, 20),\n",
    "    (3, None, 12, 8, 15),\n",
    "    (4, 'Toyota Rav4', None, 15, None),\n",
    "    (5, 'Kia Sportage', None, None, 18)\n",
    "]\n",
    "\n",
    "# Create DataFrame for FOOTER\n",
    "df_footer = spark.createDataFrame(data=footer_data, schema=footer_schema)\n",
    "df_footer.show()\n",
    "df_footer.createOrReplaceTempView(\"footer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "with car as (\n",
    "    select distinct first_value(car, true) over(order by id desc rows between unbounded preceding and unbounded following) as car from footer\n",
    "),\n",
    "length as (\n",
    "    select distinct first_value(length, true) over(order by id desc rows between unbounded preceding and unbounded following) as length from footer\n",
    "),\n",
    "width as (\n",
    "    select distinct first_value(width, true) over(order by id desc rows between unbounded preceding and unbounded following)  as width from footer\n",
    "),\n",
    "height as (\n",
    "    select distinct first_value(height, true) over(order by id desc rows between unbounded preceding and unbounded following) as height from footer\n",
    ")\n",
    "select * from car cross join length cross join width cross join height\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schema for salary DataFrame\n",
    "salary_schema = StructType([\n",
    "    StructField(\"emp_id\", IntegerType(), True),\n",
    "    StructField(\"emp_name\", StringType(), True),\n",
    "    StructField(\"base_salary\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Data for salary DataFrame\n",
    "salary_data = [\n",
    "    (1, 'Rohan', 5000),\n",
    "    (2, 'Alex', 6000),\n",
    "    (3, 'Maryam', 7000)\n",
    "]\n",
    "\n",
    "# Create DataFrame for salary\n",
    "df_salary = spark.createDataFrame(data=salary_data, schema=salary_schema)\n",
    "df_salary.show()\n",
    "\n",
    "# Define schema for income DataFrame\n",
    "income_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"income\", StringType(), True),\n",
    "    StructField(\"percentage\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Data for income DataFrame\n",
    "income_data = [\n",
    "    (1, 'Basic', 100),\n",
    "    (2, 'Allowance', 4),\n",
    "    (3, 'Others', 6)\n",
    "]\n",
    "\n",
    "# Create DataFrame for income\n",
    "df_income = spark.createDataFrame(data=income_data, schema=income_schema)\n",
    "df_income.show()\n",
    "\n",
    "# Define schema for deduction DataFrame\n",
    "deduction_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"deduction\", StringType(), True),\n",
    "    StructField(\"percentage\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Data for deduction DataFrame\n",
    "deduction_data = [\n",
    "    (1, 'Insurance', 5),\n",
    "    (2, 'Health', 6),\n",
    "    (3, 'House', 4)\n",
    "]\n",
    "\n",
    "# Create DataFrame for deduction\n",
    "df_deduction = spark.createDataFrame(data=deduction_data, schema=deduction_schema)\n",
    "df_deduction.show()\n",
    "\n",
    "# Create a temporary view for emp_transaction\n",
    "df_salary.createOrReplaceTempView(\"salary\")\n",
    "df_income.createOrReplaceTempView(\"income\")\n",
    "df_deduction.createOrReplaceTempView(\"deduction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "with income_transactions as (\n",
    "    select emp_id, emp_name, income as trans_type, base_salary * (percentage/100) as amount\n",
    "    from salary cross join income\n",
    "),\n",
    "deduction_transactions as (\n",
    "    select emp_id, emp_name, deduction as trans_type, base_salary * (percentage/100) as amount\n",
    "    from salary cross join deduction\n",
    ")\n",
    "\n",
    "select * from income_transactions union all\n",
    "select * from deduction_transactions\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "with income_transactions as (\n",
    "    select emp_id, emp_name, income as trans_type, base_salary * (percentage/100) as amount\n",
    "    from salary cross join income\n",
    "),\n",
    "deduction_transactions as (\n",
    "    select emp_id, emp_name, deduction as trans_type, base_salary * (percentage/100) as amount\n",
    "    from salary cross join deduction\n",
    "),\n",
    "emp_transactions as (\n",
    "    select * from income_transactions union all\n",
    "    select * from deduction_transactions\n",
    ")\n",
    "select emp_name, \n",
    "sum(if(trans_type = 'Basic', amount, 0)) as Basic,\n",
    "sum(if(trans_type = 'Allowance', amount, 0)) as Allowance,\n",
    "sum(if(trans_type = 'Others', amount, 0)) as Others,\n",
    "sum(if(trans_type in ('Basic', 'Allowance', 'Others') , amount, 0)) as Gross,\n",
    "sum(if(trans_type = 'Insurance', amount, 0)) as Insurance,\n",
    "sum(if(trans_type = 'Health', amount, 0)) as Health,\n",
    "sum(if(trans_type = 'House', amount, 0)) as House,\n",
    "sum(if(trans_type in ('Insurance', 'Health', 'House') , amount, 0)) as Total_Deduction,\n",
    "sum(if(trans_type in ('Basic', 'Allowance', 'Others') , amount, 0)) - sum(if(trans_type in ('Insurance', 'Health', 'House') , amount, 0)) as Netpay\n",
    "from emp_transactions group by 1\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schema for student_tests DataFrame\n",
    "student_tests_schema = StructType([\n",
    "    StructField(\"test_id\", IntegerType(), True),\n",
    "    StructField(\"marks\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Data for student_tests DataFrame\n",
    "student_tests_data = [\n",
    "    (100, 55),\n",
    "    (101, 55),\n",
    "    (102, 60),\n",
    "    (103, 58),\n",
    "    (104, 40),\n",
    "    (105, 50)\n",
    "]\n",
    "\n",
    "# Create DataFrame for student_tests\n",
    "df_student_tests = spark.createDataFrame(data=student_tests_data, schema=student_tests_schema)\n",
    "df_student_tests.show()\n",
    "df_student_tests.createOrReplaceTempView(\"student_tests\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "with cte as (\n",
    "    select test_id, marks, lag(marks,1,-1) over(partition by 1 order by test_id) as prev\n",
    "    from student_tests\n",
    ")\n",
    "\n",
    "select test_id, marks from cte where marks > prev\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Define schema for Day_Indicator DataFrame\n",
    "day_indicator_schema = StructType([\n",
    "    StructField(\"Product_ID\", StringType(), True),\n",
    "    StructField(\"Day_Indicator\", StringType(), True),\n",
    "    StructField(\"Dates\", DateType(), True)\n",
    "])\n",
    "\n",
    "# Data for Day_Indicator DataFrame\n",
    "day_indicator_data = [\n",
    "    ('AP755', '1010101', datetime.strptime('04-Mar-2024', '%d-%b-%Y')),\n",
    "    ('AP755', '1010101', datetime.strptime('05-Mar-2024', '%d-%b-%Y')),\n",
    "    ('AP755', '1010101', datetime.strptime('06-Mar-2024', '%d-%b-%Y')),\n",
    "    ('AP755', '1010101', datetime.strptime('07-Mar-2024', '%d-%b-%Y')),\n",
    "    ('AP755', '1010101', datetime.strptime('08-Mar-2024', '%d-%b-%Y')),\n",
    "    ('AP755', '1010101', datetime.strptime('09-Mar-2024', '%d-%b-%Y')),\n",
    "    ('AP755', '1010101', datetime.strptime('10-Mar-2024', '%d-%b-%Y')),\n",
    "    ('XQ802', '1000110', datetime.strptime('04-Mar-2024', '%d-%b-%Y')),\n",
    "    ('XQ802', '1000110', datetime.strptime('05-Mar-2024', '%d-%b-%Y')),\n",
    "    ('XQ802', '1000110', datetime.strptime('06-Mar-2024', '%d-%b-%Y')),\n",
    "    ('XQ802', '1000110', datetime.strptime('07-Mar-2024', '%d-%b-%Y')),\n",
    "    ('XQ802', '1000110', datetime.strptime('08-Mar-2024', '%d-%b-%Y')),\n",
    "    ('XQ802', '1000110', datetime.strptime('09-Mar-2024', '%d-%b-%Y')),\n",
    "    ('XQ802', '1000110', datetime.strptime('10-Mar-2024', '%d-%b-%Y'))\n",
    "]\n",
    "\n",
    "# Create DataFrame for Day_Indicator\n",
    "df_day_indicator = spark.createDataFrame(data=day_indicator_data, schema=day_indicator_schema)\n",
    "df_day_indicator.show()\n",
    "df_day_indicator.createOrReplaceTempView(\"day_indicator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "with cte as (\n",
    "    select Product_ID,\n",
    "    Dates,\n",
    "    if(substr(Day_Indicator,0,1) = 1, 2, 0) as Mon,\n",
    "    if(substr(Day_Indicator,2,1) = 1, 3, 0) as Tue,\n",
    "    if(substr(Day_Indicator,3,1) = 1, 4, 0) as Wed,\n",
    "    if(substr(Day_Indicator,4,1) = 1, 5, 0) as Thu,\n",
    "    if(substr(Day_Indicator,5,1) = 1, 6, 0) as Fri,\n",
    "    if(substr(Day_Indicator,6,1) = 1, 7, 0) as Sat,\n",
    "    if(substr(Day_Indicator,7,1) = 1, 1, 0) as Sun,\n",
    "    dayOfWeek(Dates) as day\n",
    "    from day_indicator\n",
    ")\n",
    "select Product_ID, Dates from cte \n",
    "where \n",
    "Mon = day or \n",
    "Tue = day or \n",
    "Wed = day or \n",
    "Thu = day or \n",
    "Fri = day or \n",
    "Sat = day or \n",
    "Sun = day\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schema for job_skills DataFrame\n",
    "job_skills_schema = StructType([\n",
    "    StructField(\"row_id\", IntegerType(), True),\n",
    "    StructField(\"job_role\", StringType(), True),\n",
    "    StructField(\"skills\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Data for job_skills DataFrame\n",
    "job_skills_data = [\n",
    "    (1, 'Data Engineer', 'SQL'),\n",
    "    (2, None, 'Python'),\n",
    "    (3, None, 'AWS'),\n",
    "    (4, None, 'Snowflake'),\n",
    "    (5, None, 'Apache Spark'),\n",
    "    (6, 'Web Developer', 'Java'),\n",
    "    (7, None, 'HTML'),\n",
    "    (8, None, 'CSS'),\n",
    "    (9, 'Data Scientist', 'Python'),\n",
    "    (10, None, 'Machine Learning'),\n",
    "    (11, None, 'Deep Learning'),\n",
    "    (12, None, 'Tableau')\n",
    "]\n",
    "\n",
    "# Create DataFrame for job_skills\n",
    "df_job_skills = spark.createDataFrame(data=job_skills_data, schema=job_skills_schema)\n",
    "df_job_skills.show()\n",
    "df_job_skills.createOrReplaceTempView(\"job_skills\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "with cte as (\n",
    "    select *, sum(if(job_role is not null, 1, 0)) over(partition by 1 order by row_id) as segment\n",
    "    from job_skills\n",
    ")\n",
    "select row_id, first_value(job_role,true) over(partition by segment) as job_role , skills from cte\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schema for orders DataFrame\n",
    "orders_schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"dates\", StringType(), True),\n",
    "    StructField(\"product_id\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Data for orders DataFrame\n",
    "orders_data = [\n",
    "    (1, '2024-02-18', 101),\n",
    "    (1, '2024-02-18', 102),\n",
    "    (1, '2024-02-19', 101),\n",
    "    (1, '2024-02-19', 103),\n",
    "    (2, '2024-02-18', 104),\n",
    "    (2, '2024-02-18', 105),\n",
    "    (2, '2024-02-19', 101),\n",
    "    (2, '2024-02-19', 106)\n",
    "]\n",
    "\n",
    "# Create DataFrame for orders\n",
    "df_orders = spark.createDataFrame(data=orders_data, schema=orders_schema)\n",
    "df_orders.show()\n",
    "df_orders.createOrReplaceTempView(\"orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "with cte as (\n",
    "    select dates, concat_ws(\",\", collect_list(product_id)) \n",
    "    from orders\n",
    "    group by customer_id, dates\n",
    ")\n",
    "select * from  cte\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|Friend1|Friend2|\n",
      "+-------+-------+\n",
      "|  Jason|   Mary|\n",
      "|   Mike|   Mary|\n",
      "|   Mike|  Jason|\n",
      "|  Susan|  Jason|\n",
      "|   John|   Mary|\n",
      "|  Susan|   Mary|\n",
      "+-------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create a list of tuples with your data\n",
    "data = [\n",
    "    ('Jason', 'Mary'),\n",
    "    ('Mike', 'Mary'),\n",
    "    ('Mike', 'Jason'),\n",
    "    ('Susan', 'Jason'),\n",
    "    ('John', 'Mary'),\n",
    "    ('Susan', 'Mary')\n",
    "]\n",
    "\n",
    "# Define the schema\n",
    "schema = [\"Friend1\", \"Friend2\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Register DataFrame as a temporary view\n",
    "df.createOrReplaceTempView(\"Friends\")\n",
    "\n",
    "# Verify the DataFrame\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:===========================================>              (6 + 2) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------------+\n",
      "|Friend1|friends                   |\n",
      "+-------+--------------------------+\n",
      "|Jason  |[Mike, Susan, Mary]       |\n",
      "|Susan  |[Jason, Mary]             |\n",
      "|Mary   |[Mike, Jason, Susan, John]|\n",
      "|Mike   |[Jason, Mary]             |\n",
      "|John   |[Mary]                    |\n",
      "+-------+--------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/12 06:50:27 ERROR TaskSchedulerImpl: Lost executor 1 on 172.18.0.3: worker lost: Not receiving heartbeat for 60 seconds\n",
      "24/09/12 06:50:27 ERROR TaskSchedulerImpl: Lost executor 0 on 172.18.0.5: worker lost: Not receiving heartbeat for 60 seconds\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "with cte as (\n",
    "select Friend1, collect_set(Friend2) as friends from (select Friend1, Friend2 from Friends \n",
    "                union all select Friend2, Friend1 from Friends)x\n",
    "group by Friend1\n",
    "    \n",
    ")\n",
    "select * from  cte\n",
    "\n",
    "\"\"\").show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
